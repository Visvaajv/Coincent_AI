{
  "cells": [
    {
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'sentiment140:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2477%2F4140%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240728%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240728T133005Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1e19c469f811730851e99e88114bf2c4de3af9975961c1c303fa02b7a8c5fae5ae6d2e11fad3fed4bd7ccfec5839fc3c2ee5b37d4f4ef32fca6b6cefce1dc09c17dbe6701643917d501fed778a8b799800cd063ed4b369ad6f90151de595c1821016d74661e56ee582869d38ad91a2a215f25a041924a882df8786865c614542f4dd221776944c4f31b7d7098a5a78f5b71cfe43c7f51dc29a17db9c7270814094a1ccb3db7ebcda2adcdea2ad3c2b17634baf7e55c153c39071b06e1bf14b2090235a4cee903abcfafd51673c2a7e344533c92527ed373b2327b4dadd33573e6c79ac07e77b8f2547f1856d77dcab9e49a15262edc1db26493c7384d2d38c2a'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "DBLYbHagFmnb"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "nve7yak2Fmns"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "print(\"Tensorflow Version\",tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "hcAZhBvhFmnu"
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv',\n",
        "                 encoding = 'latin',header=None)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-4v4PtFvFmnw"
      },
      "cell_type": "code",
      "source": [
        "df.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ScQjQWiaFmnx"
      },
      "cell_type": "code",
      "source": [
        "df = df.drop(['id', 'date', 'query', 'user_id'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "RVFjuryaFmny"
      },
      "cell_type": "code",
      "source": [
        "lab_to_sentiment = {0:\"Negative\", 4:\"Positive\"}\n",
        "def label_decoder(label):\n",
        "  return lab_to_sentiment[label]\n",
        "df.sentiment = df.sentiment.apply(lambda x: label_decoder(x))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8NqPbC4pFmnz"
      },
      "cell_type": "code",
      "source": [
        "val_count = df.sentiment.value_counts()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(val_count.index, val_count.values)\n",
        "plt.title(\"Sentiment Data Distribution\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zb0iKShwFmnz"
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "random_idx_list = [random.randint(1,len(df.text)) for i in range(10)] # creates random indexes to choose from dataframe\n",
        "df.loc[random_idx_list,:].head(10) # Returns the rows with the index and display it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "86h2_wh-Fmn0"
      },
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "2tni0tRgFmn1"
      },
      "cell_type": "code",
      "source": [
        "def preprocess(text, stem=False):\n",
        "  text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
        "  tokens = []\n",
        "  for token in text.split():\n",
        "    if token not in stop_words:\n",
        "      if stem:\n",
        "        tokens.append(stemmer.stem(token))\n",
        "      else:\n",
        "        tokens.append(token)\n",
        "  return \" \".join(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "tOny20zLFmn1"
      },
      "cell_type": "code",
      "source": [
        "df.text = df.text.apply(lambda x: preprocess(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p-xBIdXoFmn2"
      },
      "cell_type": "markdown",
      "source": [
        "### Positive Words"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "QCMd8CssFmn2"
      },
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "plt.figure(figsize = (20,20))\n",
        "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.sentiment == 'Positive'].text))\n",
        "plt.imshow(wc , interpolation = 'bilinear')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P6c437auFmn2"
      },
      "cell_type": "markdown",
      "source": [
        "### Negative Words"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "a_zqBN6vFmn2"
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,20))\n",
        "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.sentiment == 'Negative'].text))\n",
        "plt.imshow(wc , interpolation = 'bilinear')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s_Bt-vf5Fmn3"
      },
      "cell_type": "markdown",
      "source": [
        "## Train and Test Split"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "FI_MNwanFmn3"
      },
      "cell_type": "code",
      "source": [
        "TRAIN_SIZE = 0.8\n",
        "MAX_NB_WORDS = 100000\n",
        "MAX_SEQUENCE_LENGTH = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "5D7kvnSVFmn3"
      },
      "cell_type": "code",
      "source": [
        "train_data, test_data = train_test_split(df, test_size=1-TRAIN_SIZE,\n",
        "                                         random_state=7) # Splits Dataset into Training and Testing set\n",
        "print(\"Train Data size:\", len(train_data))\n",
        "print(\"Test Data size\", len(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xJKyrmUgFmn3"
      },
      "cell_type": "markdown",
      "source": [
        "`train_test_split` will shuffle the dataset and split it to gives training and testing dataset. It's important to shuffle our dataset before training."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "MVF6pqFjFmn4"
      },
      "cell_type": "code",
      "source": [
        "train_data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "EPps4B-xFmn5"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data.text)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary Size :\", vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "707t_zVwFmn8"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(train_data.text),\n",
        "                        maxlen = MAX_SEQUENCE_LENGTH)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(test_data.text),\n",
        "                       maxlen = MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"Training X Shape:\",x_train.shape)\n",
        "print(\"Testing X Shape:\",x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ly0vkdY4Fmn8"
      },
      "cell_type": "code",
      "source": [
        "labels = train_data.sentiment.unique().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZK2tB7v_Fmn8"
      },
      "cell_type": "markdown",
      "source": [
        "### Label Encoding\n",
        "We are building the model to predict class in enocoded form (0 or 1 as this is a binary classification). We should encode our training labels to encodings."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "xiFcRBhMFmn9"
      },
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "encoder.fit(train_data.sentiment.to_list())\n",
        "\n",
        "y_train = encoder.transform(train_data.sentiment.to_list())\n",
        "y_test = encoder.transform(test_data.sentiment.to_list())\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "C4GPi2TtFmn9"
      },
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "h1JOe0zHFmn9"
      },
      "cell_type": "code",
      "source": [
        "GLOVE_EMB = '/kaggle/working/glove.6B.300d.txt'\n",
        "EMBEDDING_DIM = 300\n",
        "LR = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EPOCHS = 10\n",
        "MODEL_PATH = '.../output/kaggle/working/best_model.hdf5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3Dh7zL9BFmoE"
      },
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "\n",
        "f = open(GLOVE_EMB)\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = value = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' %len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "625gXM_mFmoE"
      },
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "pYCDfqwrFmoE"
      },
      "cell_type": "code",
      "source": [
        "embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          EMBEDDING_DIM,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                          trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "whh_E6c4FmoF"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "I4wVVlrGFmoG"
      },
      "cell_type": "code",
      "source": [
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedding_sequences = embedding_layer(sequence_input)\n",
        "x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "x = Conv1D(64, 5, activation='relu')(x)\n",
        "x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(sequence_input, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-oqWD72MFmoG"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "ReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n",
        "                                     min_lr = 0.01,\n",
        "                                     monitor = 'val_loss',\n",
        "                                     verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "SERNQytNFmoI"
      },
      "cell_type": "code",
      "source": [
        "print(\"Training on GPU...\") if tf.test.is_gpu_available() else print(\"Training on CPU...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "4B25mta_FmoI"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "kU1MRp8BFmoK"
      },
      "cell_type": "code",
      "source": [
        "s, (at, al) = plt.subplots(2,1)\n",
        "at.plot(history.history['accuracy'], c= 'b')\n",
        "at.plot(history.history['val_accuracy'], c='r')\n",
        "at.set_title('model accuracy')\n",
        "at.set_ylabel('accuracy')\n",
        "at.set_xlabel('epoch')\n",
        "at.legend(['LSTM_train', 'LSTM_val'], loc='upper left')\n",
        "\n",
        "al.plot(history.history['loss'], c='m')\n",
        "al.plot(history.history['val_loss'], c='c')\n",
        "al.set_title('model loss')\n",
        "al.set_ylabel('loss')\n",
        "al.set_xlabel('epoch')\n",
        "al.legend(['train', 'val'], loc = 'upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PJRw6b67FmoM"
      },
      "cell_type": "markdown",
      "source": [
        "The model will output a prediction score between 0 and 1. We can classify two classes by defining a threshold value for it. In our case, I have set 0.5 as THRESHOLD value, if the score above it. Then it will be classified as **POSITIVE** sentiment."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-VJ0XuUXFmoN"
      },
      "cell_type": "code",
      "source": [
        "def decode_sentiment(score):\n",
        "    return \"Positive\" if score>0.5 else \"Negative\"\n",
        "\n",
        "\n",
        "scores = model.predict(x_test, verbose=1, batch_size=10000)\n",
        "y_pred_1d = [decode_sentiment(score) for score in scores]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UrE7Ra8AFmoN"
      },
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix\n",
        "Confusion Matrix provide a nice overlook at the model's performance in classification task"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "23jn6bMjFmoO"
      },
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title, fontsize=20)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, fontsize=13)\n",
        "    plt.yticks(tick_marks, classes, fontsize=13)\n",
        "\n",
        "    fmt = '.2f'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label', fontsize=17)\n",
        "    plt.xlabel('Predicted label', fontsize=17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Pav-0ffVFmoO"
      },
      "cell_type": "code",
      "source": [
        "cnf_matrix = confusion_matrix(test_data.sentiment.to_list(), y_pred_1d)\n",
        "plt.figure(figsize=(6,6))\n",
        "plot_confusion_matrix(cnf_matrix, classes=test_data.sentiment.unique(), title=\"Confusion matrix\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IuVokQuaFmoO"
      },
      "cell_type": "markdown",
      "source": [
        "### Classification Scores"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Z0nokwpPFmoP"
      },
      "cell_type": "code",
      "source": [
        "print(classification_report(list(test_data.sentiment), y_pred_1d))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EP6nvqwMFmoP"
      },
      "cell_type": "markdown",
      "source": [
        "It's a pretty good model we trained here in terms of NLP. Around 80% accuracy is good enough considering the baseline human accuracy also pretty low in these tasks. Also, you may go on and explore the dataset, some tweets might have other languages than English. So our Embedding and Tokenizing wont have effect on them. But on practical scenario, this model is good for handling most tasks for Sentiment Analysis."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}